{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5709224e-a720-4bbe-8cf3-61aeeb34cd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!c1.4\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import (\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    GlobalAveragePooling2D,\n",
    "    MaxPool2D,\n",
    ")\n",
    "from keras.models import Model, Sequential\n",
    "from keras.preprocessing import image\n",
    "from keras.utils import to_categorical\n",
    "from PIL import Image, ImageChops, ImageEnhance\n",
    "from scipy import stats\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.applications import ResNet50, Xception\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data_prepare.dataset_tools import extract_zip_with_cleanup, prepare_and_save_data, create_data_generators\n",
    "from data_prepare.plots import plot_history, confusion_matrix_plot, roc_plot, precision_recall_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b54ecb10-8f1f-4303-9d3b-808c66793572",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = \"models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1b80ea5-a511-4fdc-a683-a804a5370322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_data():\n",
    "    image_archive_path = \"data/celeb/v1/\"\n",
    "    fake_images_path, real_images_path = extract_zip_with_cleanup(image_archive_path)\n",
    "    print(fake_images_path, real_images_path)\n",
    "    train_dir, val_dir, test_dir = prepare_and_save_data(real_images_path, fake_images_path, output_dir=\"data/dataset/x\")\n",
    "    return create_data_generators(train_dir, val_dir, test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ce17253-3bce-422d-9f63-17e6782a2f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/celeb/v1/fake data/celeb/v1/real\n",
      "Found 9073 images belonging to 2 classes.\n",
      "Found 1944 images belonging to 2 classes.\n",
      "Found 1946 images belonging to 2 classes.\n",
      "\n",
      "Class indices: {'fake': 0, 'real': 1}\n",
      "Train samples: 9073\n",
      "Val samples: 1944\n",
      "Test samples: 1946\n"
     ]
    }
   ],
   "source": [
    "train_gen, val_gen, test_gen = init_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33f17565-2474-4d8a-a1a6-790bdecb8d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_xception_model(input_shape=(299, 299, 3), num_classes=1):\n",
    "    \"\"\"\n",
    "    Строит модель на основе Xception для обнаружения дипфейков\n",
    "\n",
    "    Параметры:\n",
    "        input_shape: размер входного изображения (по умолчанию 299x299 для Xception)\n",
    "        num_classes: 1 для бинарной классификации (sigmoid), 2 для softmax\n",
    "    \"\"\"\n",
    "\n",
    "    base_model = Xception(\n",
    "        weights=\"imagenet\", include_top=False, input_shape=input_shape\n",
    "    )\n",
    "\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = Sequential(\n",
    "        [\n",
    "            base_model,\n",
    "            GlobalAveragePooling2D(),\n",
    "            BatchNormalization(),\n",
    "            Dense(256, activation=\"relu\"),\n",
    "            Dropout(0.5),\n",
    "            Dense(num_classes, activation=\"sigmoid\" if num_classes == 1 else \"softmax\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-4),\n",
    "        loss=\"binary_crossentropy\" if num_classes == 1 else \"categorical_crossentropy\",\n",
    "        metrics=[\n",
    "            tf.keras.metrics.Precision(name=\"precision\"),\n",
    "            tf.keras.metrics.Recall(name=\"recall\"),\n",
    "            tf.keras.metrics.AUC(name=\"auc\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return model, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5dfb05ee-cabe-4dba-bc28-4257dec90af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_generator, val_generator, input_size=(224, 224), unfreeze_layers=0, initial_epochs=5, total_epochs=15):\n",
    "    \"\"\"Обучение с подбором LR и визуализацией\"\"\"\n",
    "    model, base_model = build_xception_model(\n",
    "        input_shape=(*input_size, 3)\n",
    "    )\n",
    "\n",
    "    model_path = os.path.join(models_dir, f'best_model_{input_size[0]}x{input_size[1]}_unfreeze{unfreeze_layers}.h5')\n",
    "    callbacks = [\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=3,\n",
    "            min_lr=1e-7,\n",
    "            verbose=1\n",
    "        ),\n",
    "        EarlyStopping(\n",
    "            monitor='val_auc',\n",
    "            patience=5,\n",
    "            mode='max',\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            model_path,\n",
    "            monitor='val_auc',\n",
    "            save_best_only=True,\n",
    "            mode='max'\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=initial_epochs,\n",
    "        validation_data=val_generator,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    \n",
    "    if not fine_tune:\n",
    "        return model, history\n",
    "    \n",
    "    base_model.trainable = True\n",
    "    \n",
    "    for layer in base_model.layers[:int(len(base_model.layers)*unfreeze_layers)]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=1e-5),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[\n",
    "            tf.keras.metrics.Precision(name=\"precision\"),\n",
    "            tf.keras.metrics.Recall(name=\"recall\"),\n",
    "            tf.keras.metrics.AUC(name=\"auc\"),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    fine_tune_history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=initial_epochs + fine_tune_epochs,\n",
    "        initial_epoch=history.epoch[-1] + 1,\n",
    "        validation_data=val_generator,\n",
    "        callbacks=[\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8328035e-c86b-4cc5-a69c-fcd64768d7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_gen, input_size, unfreeze_layers, history):\n",
    "    \"\"\"Оценка и визуализация результатов с уникальными именами файлов\"\"\"\n",
    "    # Создаем папку для результатов, если её нет\n",
    "    os.makedirs(\"evaluation_plots\", exist_ok=True)\n",
    "    \n",
    "    # 1. Генерируем уникальный префикс для файлов\n",
    "    prefix = f\"size_{input_size[0]}x{input_size[1]}_unfreeze_{unfreeze_layers}\"\n",
    "    \n",
    "    # 2. Confusion Matrix\n",
    "    y_pred = model.predict(test_gen)\n",
    "    y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "    y_true = test_gen.labels\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred_classes)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['Fake', 'Real'],\n",
    "                yticklabels=['Fake', 'Real'])\n",
    "    plt.title(f'Confusion Matrix\\n{prefix}')\n",
    "    plt.savefig(f'evaluation_plots/{prefix}_confusion_matrix.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['auc'], label='Train AUC')\n",
    "    plt.plot(history.history['val_auc'], label='Val AUC')\n",
    "    plt.title('AUC Curve')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title('Loss Curve')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.suptitle(f'Learning Curves - {prefix}')\n",
    "    plt.savefig(f'evaluation_plots/{prefix}_learning_curves.png', bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # 4. График LR (если использовался ReduceLROnPlateau)\n",
    "    if 'lr' in history.history:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(history.history['lr'])\n",
    "        plt.title('Learning Rate Schedule')\n",
    "        plt.ylabel('Learning Rate')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.savefig(f'evaluation_plots/{prefix}_lr_schedule.png', bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    return {\n",
    "        'accuracy': model.evaluate(test_gen, verbose=0)[1],\n",
    "        'auc': model.evaluate(test_gen, verbose=0)[2]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "711979ca-59ba-4aac-83ab-918cdbbe5ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with size=(156, 159), unfreeze=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleksandr/opt/anaconda3/envs/deepfakedetection/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (299,299,3) into shape (156,159,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m train_gen.image_shape = size + (\u001b[32m3\u001b[39m,)\n\u001b[32m     11\u001b[39m val_gen.image_shape = size + (\u001b[32m3\u001b[39m,)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m model, history = train_model(\n\u001b[32m     14\u001b[39m     train_gen,\n\u001b[32m     15\u001b[39m     val_gen,\n\u001b[32m     16\u001b[39m     input_size=size,\n\u001b[32m     17\u001b[39m     unfreeze_layers=unfreeze\n\u001b[32m     18\u001b[39m )\n\u001b[32m     20\u001b[39m metrics = evaluate_model(\n\u001b[32m     21\u001b[39m     model, \n\u001b[32m     22\u001b[39m     test_gen,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     history=history\n\u001b[32m     26\u001b[39m )\n\u001b[32m     28\u001b[39m results.append({\n\u001b[32m     29\u001b[39m     \u001b[33m'\u001b[39m\u001b[33minput_size\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     30\u001b[39m     \u001b[33m'\u001b[39m\u001b[33munfreeze_layers\u001b[39m\u001b[33m'\u001b[39m: unfreeze,\n\u001b[32m   (...)\u001b[39m\u001b[32m     33\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mplots_folder\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mevaluation_plots\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     34\u001b[39m })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(train_generator, val_generator, input_size, unfreeze_layers, initial_epochs, total_epochs)\u001b[39m\n\u001b[32m      7\u001b[39m model_path = os.path.join(models_dir, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mbest_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_size[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mx\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_size[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_unfreeze\u001b[39m\u001b[38;5;132;01m{\u001b[39;00munfreeze_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.h5\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m callbacks = [\n\u001b[32m      9\u001b[39m     ReduceLROnPlateau(\n\u001b[32m     10\u001b[39m         monitor=\u001b[33m'\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     27\u001b[39m     )\n\u001b[32m     28\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m history = model.fit(\n\u001b[32m     31\u001b[39m     train_generator,\n\u001b[32m     32\u001b[39m     epochs=initial_epochs,\n\u001b[32m     33\u001b[39m     validation_data=val_generator,\n\u001b[32m     34\u001b[39m     callbacks=callbacks\n\u001b[32m     35\u001b[39m )\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fine_tune:\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/deepfakedetection/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/deepfakedetection/lib/python3.12/site-packages/keras/src/legacy/preprocessing/image.py:329\u001b[39m, in \u001b[36mBatchFromFilesMixin._get_batches_of_transformed_samples\u001b[39m\u001b[34m(self, index_array)\u001b[39m\n\u001b[32m    327\u001b[39m         x = \u001b[38;5;28mself\u001b[39m.image_data_generator.apply_transform(x, params)\n\u001b[32m    328\u001b[39m         x = \u001b[38;5;28mself\u001b[39m.image_data_generator.standardize(x)\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     batch_x[i] = x\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# optionally save augmented images to disk for debugging purposes\u001b[39;00m\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.save_to_dir:\n",
      "\u001b[31mValueError\u001b[39m: could not broadcast input array from shape (299,299,3) into shape (156,159,3)"
     ]
    }
   ],
   "source": [
    "input_sizes = [(156, 159), (224, 224), (299, 299)]\n",
    "unfreeze_options = [0, 30, 100]\n",
    "\n",
    "results = []\n",
    "for size in input_sizes:\n",
    "    for unfreeze in unfreeze_options:\n",
    "        print(f\"\\nTraining with size={size}, unfreeze={unfreeze}\")\n",
    "        \n",
    "        # Адаптируем генераторы под новый размер\n",
    "        train_gen.image_shape = size + (3,)\n",
    "        val_gen.image_shape = size + (3,)\n",
    "        \n",
    "        model, history = train_model(\n",
    "            train_gen,\n",
    "            val_gen,\n",
    "            input_size=size,\n",
    "            unfreeze_layers=unfreeze\n",
    "        )\n",
    "        \n",
    "        metrics = evaluate_model(\n",
    "            model, \n",
    "            test_gen,\n",
    "            input_size=size,\n",
    "            unfreeze_layers=unfreeze,\n",
    "            history=history\n",
    "        )\n",
    "        \n",
    "        results.append({\n",
    "            'input_size': f\"{size[0]}x{size[1]}\",\n",
    "            'unfreeze_layers': unfreeze,\n",
    "            'val_auc': max(history.history['val_auc']),\n",
    "            'test_auc': metrics['auc'],\n",
    "            'plots_folder': 'evaluation_plots'\n",
    "        })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
